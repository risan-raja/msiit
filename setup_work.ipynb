{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7ea6e3-0777-40f8-873c-3eb82e760851",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac7cff0a-7d7b-4797-8455-1e62f0bf7b32",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.023308,
     "end_time": "2022-09-22T12:45:30.854323",
     "exception": false,
     "start_time": "2022-09-22T12:45:30.831015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "import copy\n",
    "from sklearnex import patch_sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import (\n",
    "    BackwardDifferenceEncoder,\n",
    "    BaseNEncoder,\n",
    "    BinaryEncoder,\n",
    "    CatBoostEncoder,\n",
    "    CountEncoder,\n",
    "    GLMMEncoder,\n",
    "    HelmertEncoder,\n",
    "    JamesSteinEncoder,\n",
    "    LeaveOneOutEncoder,\n",
    "    MEstimateEncoder,\n",
    "    SummaryEncoder,\n",
    "    TargetEncoder,\n",
    "    WOEEncoder,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import uuid\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn import set_config\n",
    "from sklearn.base import clone as model_clone\n",
    "from sklearn.cluster import *\n",
    "from sklearn.impute import *\n",
    "from sklearn.compose import *\n",
    "from sklearn.cross_decomposition import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.feature_selection import *\n",
    "from sklearn.gaussian_process import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.multioutput import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.neural_network import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.kernel_approximation import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.utils import *\n",
    "from sklearn.dummy import *\n",
    "from sklearn.semi_supervised import *\n",
    "from sklearn.discriminant_analysis import *\n",
    "from sklearn.covariance import *\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.calibration import *\n",
    "import joblib\n",
    "from pprint import pprint as pp\n",
    "\n",
    "pd.options.compute.use_numba = True\n",
    "pd.options.compute.use_numexpr = True\n",
    "pd.options.compute.use_bottleneck = True\n",
    "pd.options.display.max_columns = 90\n",
    "set_config(display=\"diagram\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline as make_imb_pipeline\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "from imblearn.over_sampling import (\n",
    "    ADASYN,\n",
    "    SMOTE,\n",
    "    RandomOverSampler,\n",
    "    SVMSMOTE,\n",
    "    SMOTENC,\n",
    "    SMOTEN,\n",
    "    BorderlineSMOTE,\n",
    "    KMeansSMOTE,\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "from joblib import parallel_backend\n",
    "from joblib.memory import Memory\n",
    "\n",
    "# patch_sklearn()\n",
    "KAGGLE_ENV = 1\n",
    "DATA_INPUT = \"/kaggle/input/marketing-strategy-personalised-offer/\"\n",
    "DATA_OUTPUT = \"/kaggle/working/\"\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "if \"mlop3n/Pycharm\" in cwd or \"u170690\" in cwd:\n",
    "    KAGGLE_ENV = 0\n",
    "    DATA_INPUT = \"kaggle/input/marketing-strategy-personalised-offer/\"\n",
    "    DATA_OUTPUT = \"kaggle/working/\"\n",
    "CACHE = Memory(DATA_OUTPUT + \"joblib\", verbose=0)\n",
    "patch_sklearn()\n",
    "\n",
    "data = pd.read_csv(DATA_INPUT + \"train_data.csv\")\n",
    "eval_data = pd.read_csv(DATA_INPUT + \"test_data.csv\")\n",
    "\n",
    "\n",
    "def gen_train_test(X, y, test_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=10\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def save_data():\n",
    "    global data, eval_data\n",
    "    data.to_parquet(DATA_OUTPUT + \"data.parquet\")\n",
    "    eval_data.to_parquet(DATA_OUTPUT + \"eval_data.parquet\")\n",
    "\n",
    "\n",
    "def quick_test(X):\n",
    "    clfs = [\n",
    "        RandomForestClassifier(class_weight=\"balanced_subsample\", random_state=42),\n",
    "        DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "        HistGradientBoostingClassifier(random_state=42),\n",
    "        LogisticRegressionCV(max_iter=10000, class_weight=\"balanced\", random_state=42),\n",
    "    ]\n",
    "    y = data.target\n",
    "    X_train, X_test, y_train, y_test = gen_train_test(X, y, test_size=0.5)\n",
    "    for clf in clfs:\n",
    "        y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "        score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        print(f\"{clf.__class__.__name__} :: {score}\")\n",
    "\n",
    "\n",
    "def check_RF_perf(X, y):\n",
    "    clf = RandomForestClassifier(\n",
    "        class_weight=\"balanced\", n_jobs=24, max_features=None, max_depth=8\n",
    "    )\n",
    "    with parallel_backend(\"threading\"):\n",
    "        scores = cross_validate(\n",
    "            clf,\n",
    "            X,\n",
    "            y,\n",
    "            cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42),\n",
    "            n_jobs=24,\n",
    "            return_train_score=True,\n",
    "            scoring=\"f1_macro\",\n",
    "        )\n",
    "    _ = plt.plot(scores[\"test_score\"], label=\"TEST\")\n",
    "    _ = plt.plot(scores[\"train_score\"], label=\"TRAIN\")\n",
    "    _ = plt.legend()\n",
    "\n",
    "\n",
    "def check_catNB_perf(X, y):\n",
    "    min_c = X.nunique().astype(\"int\").to_numpy() + 1\n",
    "    # clf = RandomForestClassifier(class_weight='balanced',\n",
    "    #                              n_jobs=24,\n",
    "    # #                              max_features=None,\n",
    "    #                              )\n",
    "    class_prior = (y.value_counts() / X.shape[0]).sort_index().to_numpy()\n",
    "    clf = CategoricalNB(\n",
    "        fit_prior=True,\n",
    "        alpha=0.0000003,\n",
    "        min_categories=min_c,\n",
    "        #                     class_prior=class_prior\n",
    "    )\n",
    "    categories_ = []\n",
    "    for c in X.columns:\n",
    "        categories_.append(sorted(list(X[c].unique())))\n",
    "\n",
    "    work = make_pipeline(OrdinalEncoder(categories=categories_), clf)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.5,\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "        stratify=y,\n",
    "    )\n",
    "    with parallel_backend(\"threading\"):\n",
    "        #     scores = cross_validate(clf,X,y,cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42),n_jobs=24,return_train_score=True,scoring='f1_macro')\n",
    "        y_pred = work.fit(X_train, y_train).predict(X_test)\n",
    "        print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cef9a14d-b80f-40f1-9cab-00f7a17eff60",
   "metadata": {
    "papermill": {
     "duration": 14.904815,
     "end_time": "2022-09-22T12:45:45.975943",
     "exception": false,
     "start_time": "2022-09-22T12:45:31.071128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bring_data():\n",
    "    data = pd.read_csv(DATA_INPUT + \"train_data.csv\")\n",
    "    eval_data = pd.read_csv(DATA_INPUT + \"test_data.csv\")\n",
    "    ques = \"Do you want to use new data?\"\n",
    "    ans = input(prompt=ques)\n",
    "    if ans.lower() == \"y\":\n",
    "\n",
    "        def convert_to_hours(row):\n",
    "            if \"hours\" in row[\"offer expiration\"]:\n",
    "                #         row['offer expiration'] = int(row['offer expiration'][:-5])\n",
    "                row[\"offer expiration\"] = 0\n",
    "            elif \"days\" in row[\"offer expiration\"]:\n",
    "                #         row['offer expiration'] = int(row['offer expiration'][:-4])*24\n",
    "                row[\"offer expiration\"] = 1\n",
    "            return row\n",
    "\n",
    "        data = data.apply(convert_to_hours, axis=1)\n",
    "        eval_data = eval_data.apply(convert_to_hours, axis=1)\n",
    "\n",
    "        def pythonise_col_names(df: pd.DataFrame):\n",
    "            col_names = list(df.columns)\n",
    "            rename_stubs = {}\n",
    "            for x in col_names:\n",
    "                if x == \"Offer Accepted\":\n",
    "                    r = \"target\"\n",
    "                    rename_stubs[x] = r\n",
    "                elif \"/\" in x:\n",
    "                    r = x.split(\"/\")[1]\n",
    "                    r = r.replace(\" \", \"_\").lower()\n",
    "                    rename_stubs[x] = r\n",
    "                elif \"-\" in x:\n",
    "                    r = x.replace(\"-\", \"_\").lower()\n",
    "                    r = r.replace(\" \", \"_\").lower()\n",
    "                    rename_stubs[x] = r\n",
    "                elif \" \" in x:\n",
    "                    r = x.replace(\" \", \"_\").lower()\n",
    "                    rename_stubs[x] = r\n",
    "                else:\n",
    "                    r = x.lower()\n",
    "                    rename_stubs[x] = r\n",
    "            return rename_stubs\n",
    "\n",
    "        data.rename(pythonise_col_names(data), inplace=True, axis=1)\n",
    "        eval_data.rename(pythonise_col_names(eval_data), inplace=True, axis=1)\n",
    "        gender_code = {\"Male\": 0, \"Female\": 1}\n",
    "\n",
    "        def binarize_gender(row):\n",
    "            global gender_code\n",
    "            row[\"gender\"] = gender_code[row[\"gender\"]]\n",
    "            return row\n",
    "\n",
    "        data = data.apply(binarize_gender, axis=1)\n",
    "        eval_data = eval_data.apply(binarize_gender, axis=1)\n",
    "\n",
    "        ages_ord = [\"below21\", \"21\", \"26\", \"31\", \"36\", \"41\", \"46\", \"50plus\"]\n",
    "        ordinal_age_codes = {x: idx + 1 for idx, x in enumerate(ages_ord)}\n",
    "        income_codes = {\n",
    "            \"Less than ₹12500\": 1,\n",
    "            \"₹12500 - ₹24999\": 2,\n",
    "            \"₹25000 - ₹37499\": 3,\n",
    "            \"₹37500 - ₹49999\": 4,\n",
    "            \"₹50000 - ₹62499\": 5,\n",
    "            \"₹62500 - ₹74999\": 6,\n",
    "            \"₹75000 - ₹87499\": 7,\n",
    "            \"₹87500 - ₹99999\": 8,\n",
    "            \"₹100000 or More\": 9,\n",
    "        }\n",
    "        ordinal_quali_codes = {\n",
    "            \"Bachelors degree\": 5,\n",
    "            \"Some college - no degree\": 3,\n",
    "            \"Graduate degree (Masters or Doctorate)\": 6,\n",
    "            \"Associates degree\": 4,\n",
    "            \"High School Graduate\": 2,\n",
    "            \"Some High School\": 1,\n",
    "        }\n",
    "        data[\"ordinal__income_range\"] = 0\n",
    "        eval_data[\"ordinal__income_range\"] = 0\n",
    "        data[\"ordinal__age\"] = 0\n",
    "        eval_data[\"ordinal__age\"] = 0\n",
    "        data[\"ordinal__qualif\"] = 0\n",
    "        eval_data[\"ordinal__qualif\"] = 0\n",
    "\n",
    "        def codify_special_ordinal_range(row):\n",
    "            global income_codes, ordinal_age_codes\n",
    "            row[\"ordinal__income_range\"] = income_codes[row[\"income_range\"]]\n",
    "            row[\"ordinal__age\"] = ordinal_age_codes[row[\"age\"]]\n",
    "            row[\"ordinal__qualif\"] = ordinal_quali_codes[row[\"qualification\"]]\n",
    "            return row\n",
    "\n",
    "        data = data.apply(codify_special_ordinal_range, axis=1)\n",
    "        eval_data = eval_data.apply(codify_special_ordinal_range, axis=1)\n",
    "\n",
    "        ordinal_category_codes = {\n",
    "            \"4~8\": 4,\n",
    "            \"less1\": 2,\n",
    "            \"never\": 1,\n",
    "            \"1~3\": 3,\n",
    "            \"gt8\": 5,\n",
    "            np.nan: np.nan,\n",
    "        }\n",
    "\n",
    "        # data = data.apply(codify_cold_drink_consumption,axis=1)\n",
    "        # eval_data = eval_data.apply(codify_cold_drink_consumption,axis=1)\n",
    "        ord_cols = [\n",
    "            \"restaur_spend_less_than20\",\n",
    "            \"no_take_aways\",\n",
    "            \"restaur_spend_greater_than20\",\n",
    "            \"no_visited_bars\",\n",
    "            \"no_visited_cold_drinks\",\n",
    "        ]\n",
    "        col_ord_names = [\"ordinal__\" + x for x in ord_cols]\n",
    "        data[col_ord_names] = 0\n",
    "        eval_data[col_ord_names] = 0\n",
    "\n",
    "        def codify_ordinal_columns(row):\n",
    "            global col_ord_names, ordinal_category_codes, ord_cols\n",
    "            for ord_col, col_ord_name in zip(ord_cols, col_ord_names):\n",
    "                try:\n",
    "                    row[col_ord_name] = int(ordinal_category_codes[row[ord_col]])\n",
    "                except ValueError:\n",
    "                    row[col_ord_name] = ordinal_category_codes[row[ord_col]]\n",
    "            return row\n",
    "\n",
    "        data = data.apply(codify_ordinal_columns, axis=1)\n",
    "        eval_data = eval_data.apply(codify_ordinal_columns, axis=1)\n",
    "\n",
    "        \"\"\"\n",
    "        Car Feature:\n",
    "        Label NaN as unknown for now\n",
    "        \"\"\"\n",
    "        data.car.fillna(\"unknown\", inplace=True)\n",
    "        eval_data.car.fillna(\"unknown\", inplace=True)\n",
    "\n",
    "        nominal_cs = [\n",
    "            \"restaurant_type\",\n",
    "            \"marital_status\",\n",
    "            #         \"climate\",\n",
    "            \"drop_location\",\n",
    "            \"job_industry\",\n",
    "            \"customer_type\",\n",
    "            \"car\",\n",
    "        ]\n",
    "        nominal_col_names = [\"nominal__\" + x for x in nominal_cs]\n",
    "        master_nominals = pd.concat(\n",
    "            [data[nominal_cs], eval_data[nominal_cs]], ignore_index=True, axis=0\n",
    "        )\n",
    "        nominal_encoder = {nc: {} for nc in nominal_cs}\n",
    "        for c in nominal_cs:\n",
    "            unique_vals = list(master_nominals[c].unique())\n",
    "            nominal_codes = {x: idx for idx, x in enumerate(unique_vals)}\n",
    "            nominal_encoder[c] = nominal_codes\n",
    "        data[nominal_col_names] = 0\n",
    "        eval_data[nominal_col_names] = 0\n",
    "        data[\"interval__season\"] = 0\n",
    "        eval_data[\"interval__season\"] = 0\n",
    "        season_code = {\"Spring\": 1, \"Summer\": 2, \"Winter\": 3}\n",
    "\n",
    "        def assign_numeric_seasons(row):\n",
    "            row[\"interval__season\"] = season_code[row[\"climate\"]]\n",
    "            return row\n",
    "\n",
    "        data = data.apply(assign_numeric_seasons, axis=1)\n",
    "        eval_data = eval_data.apply(assign_numeric_seasons, axis=1)\n",
    "\n",
    "        def codify_nominal_columns(row):\n",
    "            global nominal_col_names, nominal_encoder, nominal_cs\n",
    "            for nom_col, col_nom_name in zip(nominal_cs, nominal_col_names):\n",
    "                try:\n",
    "                    row[col_nom_name] = int(nominal_encoder[nom_col][row[nom_col]])\n",
    "                except ValueError:\n",
    "                    row[col_nom_name] = nominal_encoder[nom_col][row[nom_col]]\n",
    "            return row\n",
    "\n",
    "        data = data.apply(codify_nominal_columns, axis=1)\n",
    "        eval_data = eval_data.apply(codify_nominal_columns, axis=1)\n",
    "\n",
    "        def drop_const_and_enc_columns(df):\n",
    "            cols_to_drop = [\n",
    "                \"restaur_spend_less_than20\",\n",
    "                \"no_take_aways\",\n",
    "                \"restaur_spend_greater_than20\",\n",
    "                \"no_visited_bars\",\n",
    "                \"no_visited_cold_drinks\",\n",
    "                \"restaurant_type\",\n",
    "                \"marital_status\",\n",
    "                \"climate\",\n",
    "                \"qualification\",\n",
    "                \"drop_location\",\n",
    "                \"job_industry\",\n",
    "                \"customer_type\",\n",
    "                \"car\",\n",
    "                \"income_range\",\n",
    "                \"travelled_more_than_5mins_for_offer\",\n",
    "                \"age\",\n",
    "            ]\n",
    "            df = df.drop(cols_to_drop, axis=1)\n",
    "            return df\n",
    "\n",
    "        data = drop_const_and_enc_columns(data)\n",
    "        eval_data = drop_const_and_enc_columns(eval_data)\n",
    "\n",
    "        data_original_col_order = [\n",
    "            \"offer_expiration\",\n",
    "            \"ordinal__income_range\",\n",
    "            \"ordinal__no_visited_cold_drinks\",\n",
    "            \"travelled_more_than_15mins_for_offer\",\n",
    "            \"ordinal__restaur_spend_less_than20\",\n",
    "            \"nominal__marital_status\",\n",
    "            \"nominal__restaurant_type\",\n",
    "            \"ordinal__age\",\n",
    "            \"prefer_western_over_chinese\",\n",
    "            \"travelled_more_than_25mins_for_offer\",\n",
    "            \"ordinal__no_visited_bars\",\n",
    "            \"gender\",\n",
    "            \"nominal__car\",\n",
    "            \"restuarant_same_direction_house\",\n",
    "            \"cooks_regularly\",\n",
    "            \"nominal__customer_type\",\n",
    "            \"ordinal__qualif\",\n",
    "            \"is_foodie\",\n",
    "            \"ordinal__no_take_aways\",\n",
    "            \"nominal__job_industry\",\n",
    "            \"restuarant_opposite_direction_house\",\n",
    "            \"has_children\",\n",
    "            \"visit_restaurant_with_rating_(avg)\",\n",
    "            \"temperature\",\n",
    "            \"ordinal__restaur_spend_greater_than20\",\n",
    "            \"travel_time\",\n",
    "            \"interval__season\",\n",
    "            \"nominal__drop_location\",\n",
    "            \"prefer_home_food\",\n",
    "            \"target\",\n",
    "        ]\n",
    "        eval_data_original_col_order = copy.deepcopy(data_original_col_order)\n",
    "        eval_data_original_col_order.remove(\"target\")\n",
    "\n",
    "        data = data.loc[:, data_original_col_order]\n",
    "        eval_data = eval_data.loc[:, eval_data_original_col_order]\n",
    "        binary_features = [\n",
    "            \"offer_expiration\",\n",
    "            \"travelled_more_than_15mins_for_offer\",\n",
    "            \"prefer_western_over_chinese\",\n",
    "            \"travelled_more_than_25mins_for_offer\",\n",
    "            \"restuarant_same_direction_house\",\n",
    "            \"cooks_regularly\",\n",
    "            \"is_foodie\",\n",
    "            \"restuarant_opposite_direction_house\",\n",
    "            \"has_children\",\n",
    "            \"prefer_home_food\",\n",
    "            \"gender\",\n",
    "        ]\n",
    "        b_f_rn = {x: \"binary__\" + x for x in binary_features}\n",
    "        data.rename(b_f_rn, axis=1, inplace=True)\n",
    "        eval_data.rename(b_f_rn, axis=1, inplace=True)\n",
    "        remaining_ordinals = {\n",
    "            \"visit_restaurant_with_rating_(avg)\": \"ordinal__type_of_rest_rating\",\n",
    "            \"temperature\": \"interval__temperature\",\n",
    "            \"travel_time\": \"interval__travel_time\",\n",
    "        }\n",
    "\n",
    "        def rename_remaining_ord(df):\n",
    "            df = df.rename(remaining_ordinals, axis=1)\n",
    "            return df\n",
    "\n",
    "        data = rename_remaining_ord(data)\n",
    "        eval_data = rename_remaining_ord(eval_data)\n",
    "\n",
    "        target_codes = {\"Yes\": 1, \"No\": 0}\n",
    "\n",
    "        def binarize_target(row):\n",
    "            global target_codes\n",
    "            row[\"target\"] = target_codes[row[\"target\"]]\n",
    "            return row\n",
    "\n",
    "        data = data.apply(binarize_target, axis=1)\n",
    "        X = data[eval_data.columns]\n",
    "        y = data.target\n",
    "        X_eval = eval_data\n",
    "\n",
    "        def impute_values(X, y, X_eval):\n",
    "            imputer = IterativeImputer(\n",
    "                estimator=RandomForestClassifier(\n",
    "                    class_weight=\"balanced\", random_state=42, n_jobs=-1\n",
    "                ),\n",
    "                sample_posterior=False,\n",
    "                initial_strategy=\"most_frequent\",\n",
    "                random_state=42,\n",
    "            )\n",
    "            #     data.isna().sum()\n",
    "\n",
    "            with parallel_backend(\"threading\", n_jobs=24):\n",
    "                data_enc = imputer.fit_transform(X)\n",
    "                eval_data_enc = imputer.transform(eval_data)\n",
    "            return data_enc, eval_data_enc\n",
    "\n",
    "        data_enc, eval_data_enc = impute_values(X, y, X_eval)\n",
    "        data.loc[:, eval_data.columns] = data_enc\n",
    "        eval_data.loc[:, :] = eval_data_enc\n",
    "        data = data.astype(np.uint32)\n",
    "        eval_data = eval_data.astype(np.uint32)\n",
    "        save_data()\n",
    "    else:\n",
    "        data = pd.read_parquet(DATA_OUTPUT + \"data.parquet\")\n",
    "        eval_data = pd.read_parquet(DATA_OUTPUT + \"eval_data.parquet\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Degree Of Closeness Rather than Nominal Drop Variable.\n",
    "    \"\"\"\n",
    "\n",
    "    closeness_ranks = {0: 3, 1: 2, 2: 1}\n",
    "    if \"nominal__drop_location\" in data.columns:\n",
    "\n",
    "        def rank_closeness(row):\n",
    "            \"\"\"\n",
    "            It makes sense to rename the nominal drop Location to much more meaningful\n",
    "            Ordinal Variable based on General Closeness to target Pop.\n",
    "            \"\"\"\n",
    "            row[\"nominal__drop_location\"] = closeness_ranks[row[\"nominal__drop_location\"]]\n",
    "            return row\n",
    "\n",
    "        data = data.apply(rank_closeness, axis=1)\n",
    "        eval_data = eval_data.apply(rank_closeness, axis=1)\n",
    "        data.rename(\n",
    "            {\"nominal__drop_location\": \"ordinal__dest_distance\"}, axis=1, inplace=True\n",
    "        )\n",
    "        eval_data.rename(\n",
    "            {\"nominal__drop_location\": \"ordinal__dest_distance\"}, axis=1, inplace=True\n",
    "        )\n",
    "        data.rename(\n",
    "            {\"nominal__drop_location\": \"ordinal__dest_distance\"}, axis=1, inplace=True\n",
    "        )\n",
    "        eval_data.rename(\n",
    "            {\"nominal__drop_location\": \"ordinal__dest_distance\"}, axis=1, inplace=True\n",
    "        )\n",
    "\n",
    "\n",
    "    def html_px(chart):\n",
    "        f_name = str(uuid.uuid1())[:8] + \".html\"\n",
    "        f_dest = \"/home/u164131/msiit/charts/\"\n",
    "        with open(f_dest + f_name, \"w+\") as fp:\n",
    "            fp.write(chart.to_html())\n",
    "        return f_dest + f_name\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Variable Groups\n",
    "    \"\"\"\n",
    "    nominal = [x for x in eval_data.columns if \"nominal_\" in x]\n",
    "    ordinal = [x for x in eval_data.columns if \"ordinal_\" in x]\n",
    "    binary = [x for x in eval_data.columns if \"binary_\" in x]\n",
    "    interval = [x for x in eval_data.columns if \"interval_\" in x]\n",
    "\n",
    "    bio = [\n",
    "        \"binary__gender\",\n",
    "        \"ordinal__age\",\n",
    "    ]\n",
    "    personal_info = [\n",
    "        \"nominal__marital_status\",\n",
    "        \"binary__has_children\",\n",
    "        \"nominal__customer_type\",\n",
    "    ]\n",
    "    prefs = [\n",
    "        \"binary__prefer_home_food\",\n",
    "        \"binary__is_foodie\",\n",
    "        \"binary__prefer_western_over_chinese\",\n",
    "        \"binary__cooks_regularly\",\n",
    "    ]\n",
    "    distance = [\n",
    "        \"binary__travelled_more_than_15mins_for_offer\",\n",
    "        \"binary__travelled_more_than_25mins_for_offer\",\n",
    "        \"binary__restuarant_opposite_direction_house\",\n",
    "        \"binary__restuarant_same_direction_house\",\n",
    "    ]\n",
    "    usage_info = [\n",
    "        \"ordinal__no_visited_cold_drinks\",\n",
    "        \"ordinal__restaur_spend_less_than20\",\n",
    "        \"ordinal__restaur_spend_greater_than20\",\n",
    "        \"ordinal__no_take_aways\",\n",
    "        \"ordinal__type_of_rest_rating\",\n",
    "        \"ordinal__no_visited_bars\",\n",
    "    ]\n",
    "\n",
    "    biz_info = [\n",
    "        \"nominal__restaurant_type\",\n",
    "        \"binary__offer_expiration\",\n",
    "    ]\n",
    "    dest = [\n",
    "        \"ordinal__dest_distance\",\n",
    "        \"interval__travel_time\",\n",
    "    ]\n",
    "    weather = [\"interval__season\", \"interval__temperature\"]\n",
    "    wealth = [\n",
    "        \"nominal__car\",\n",
    "        \"nominal__job_industry\",\n",
    "        \"ordinal__income_range\",\n",
    "        \"ordinal__qualif\",\n",
    "    ]\n",
    "    f_groups = {\n",
    "        \"bio\": bio,\n",
    "        \"personal_info\": personal_info,\n",
    "        \"prefs\": prefs,\n",
    "        \"distance\": distance,\n",
    "        \"usage_info\": usage_info,\n",
    "        \"biz_info\": biz_info,\n",
    "        \"dest\": dest,\n",
    "        \"weather\": weather,\n",
    "        \"wealth\": wealth,\n",
    "    }\n",
    "    data[\"pref_profile\"] = 0\n",
    "    eval_data[\"pref_profile\"] = 0\n",
    "\n",
    "\n",
    "    def pref_profile(row):\n",
    "        profile_str = \"\"\n",
    "        for c in prefs:\n",
    "            profile_str += str(row[c])\n",
    "        profile_code = int(profile_str, 2)\n",
    "        row[\"pref_profile\"] = profile_code\n",
    "        return row\n",
    "\n",
    "\n",
    "    data = data.apply(pref_profile, axis=1)\n",
    "    eval_data = eval_data.apply(pref_profile, axis=1)\n",
    "    data.eval(\n",
    "        \"biz_type = (nominal__restaurant_type * 2) + (binary__offer_expiration)\",\n",
    "        inplace=True,\n",
    "    )\n",
    "    eval_data.eval(\n",
    "        \"biz_type = (nominal__restaurant_type * 2) + (binary__offer_expiration)\",\n",
    "        inplace=True,\n",
    "    )\n",
    "    # save_data()\n",
    "    data.rename(\n",
    "        {\"biz_type\": \"nominal__biz_type\", \"pref_profile\": \"nominal__pref_profile\"},\n",
    "        axis=1,\n",
    "        inplace=True,\n",
    "    )\n",
    "    eval_data.rename(\n",
    "        {\"biz_type\": \"nominal__biz_type\", \"pref_profile\": \"nominal__pref_profile\"},\n",
    "        axis=1,\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Spending Distribution\n",
    "    \"\"\"\n",
    "\n",
    "    data[\"nominal__spend_id\"] = 0\n",
    "    eval_data[\"nominal__spend_id\"] = 0\n",
    "    id_code = {\n",
    "        \"22\": 6,\n",
    "        \"34\": 13,\n",
    "        \"23\": 7,\n",
    "        \"33\": 12,\n",
    "        \"25\": 9,\n",
    "        \"13\": 2,\n",
    "        \"12\": 1,\n",
    "        \"32\": 11,\n",
    "        \"55\": 24,\n",
    "        \"11\": 0,\n",
    "        \"45\": 19,\n",
    "        \"24\": 8,\n",
    "        \"35\": 14,\n",
    "        \"14\": 3,\n",
    "        \"15\": 4,\n",
    "        \"31\": 10,\n",
    "        \"54\": 23,\n",
    "        \"43\": 17,\n",
    "        \"44\": 18,\n",
    "        \"21\": 5,\n",
    "        \"42\": 16,\n",
    "        \"53\": 22,\n",
    "        \"52\": 21,\n",
    "        \"51\": 20,\n",
    "        \"41\": 15,\n",
    "    }\n",
    "\n",
    "\n",
    "    def derive_spend_id(row):\n",
    "        cols = [\n",
    "            \"ordinal__restaur_spend_greater_than20\",\n",
    "            \"ordinal__restaur_spend_less_than20\",\n",
    "        ]\n",
    "        i_cde = \"\"\n",
    "        for c in cols:\n",
    "            i_cde += str(row[c])\n",
    "        row[\"nominal__spend_id\"] = id_code[i_cde]\n",
    "        return row\n",
    "\n",
    "\n",
    "    data = data.apply(derive_spend_id, axis=1)\n",
    "    eval_data = eval_data.apply(derive_spend_id, axis=1)\n",
    "\n",
    "\n",
    "    data[\"nominal__direction\"] = 0\n",
    "    eval_data[\"nominal__direction\"] = 0\n",
    "    direction_f = [\n",
    "        \"binary__restuarant_opposite_direction_house\",\n",
    "        \"binary__restuarant_same_direction_house\",\n",
    "    ]\n",
    "\n",
    "\n",
    "    def derive_directional_code(row):\n",
    "        i_cde = \"\"\n",
    "        for c in direction_f:\n",
    "            i_cde += str(row[c])\n",
    "        row[\"nominal__direction\"] = int(i_cde, 2)\n",
    "        return row\n",
    "\n",
    "\n",
    "    data = data.apply(derive_directional_code, axis=1)\n",
    "    eval_data = eval_data.apply(derive_directional_code, axis=1)\n",
    "    redundant = [\n",
    "        \"nominal__restaurant_type\",\n",
    "        \"binary__offer_expiration\",\n",
    "        \"binary__prefer_home_food\",\n",
    "        \"binary__is_foodie\",\n",
    "        \"binary__prefer_western_over_chinese\",\n",
    "        \"binary__cooks_regularly\",\n",
    "        \"binary__restuarant_opposite_direction_house\",\n",
    "        \"binary__restuarant_same_direction_house\",\n",
    "        \"ordinal__restaur_spend_greater_than20\",\n",
    "        \"ordinal__restaur_spend_less_than20\",\n",
    "    ]\n",
    "    extra_effort = [\n",
    "        \"binary__travelled_more_than_15mins_for_offer\",\n",
    "        \"binary__travelled_more_than_25mins_for_offer\",\n",
    "    ]\n",
    "    data[\"nominal__extra_travel\"] = 0\n",
    "    eval_data[\"nominal__extra_travel\"] = 0\n",
    "\n",
    "\n",
    "    def summarize_extra_effort(row):\n",
    "        i_cde = \"\"\n",
    "        for c in extra_effort:\n",
    "            i_cde += str(row[c])\n",
    "        row[\"nominal__extra_travel\"] = int(i_cde, 2)\n",
    "        return row\n",
    "\n",
    "\n",
    "    data = data.apply(summarize_extra_effort, axis=1)\n",
    "    eval_data = eval_data.apply(summarize_extra_effort, axis=1)\n",
    "\n",
    "    pd.options.compute.use_numba = False\n",
    "\n",
    "\n",
    "    def add_cust_type_to_trunc_features(df_, eval_df):\n",
    "        df = df_.copy()\n",
    "        cs_type_direc_ = (\n",
    "            data.groupby(\n",
    "                [\"nominal__customer_type\", \"nominal__extra_travel\", \"nominal__direction\"]\n",
    "            )[\"target\"].sum()\n",
    "            / data.groupby(\n",
    "                [\"nominal__customer_type\", \"nominal__extra_travel\", \"nominal__direction\"]\n",
    "            )[\"target\"].count()\n",
    "        ).sort_values(ascending=False)\n",
    "        # cs_type_direc.style.bar(axis=0,align='left')\n",
    "        category_cde = {}\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                for k in range(4):\n",
    "                    try:\n",
    "                        category_cde[str(i) + str(j) + str(k)] = cs_type_direc_.loc[i, j, k]\n",
    "                    except KeyError as e:\n",
    "                        continue\n",
    "        cs_type_direc = data.groupby(\n",
    "            [\"nominal__customer_type\", \"nominal__extra_travel\", \"nominal__direction\"]\n",
    "        )[\"target\"].count()\n",
    "        category_count = {}\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                for k in range(4):\n",
    "                    try:\n",
    "                        category_count[str(i) + str(j) + str(k)] = cs_type_direc.loc[\n",
    "                            i, j, k\n",
    "                        ]\n",
    "                    except KeyError as e:\n",
    "                        continue\n",
    "        freq = dict(Counter(list(category_cde.values())))\n",
    "        commn_ = [i for i in freq if freq[i] > 1]\n",
    "        cmmn_label_groups = {\n",
    "            cmn: [k for k in category_cde if category_cde[k] == cmn] for cmn in commn_\n",
    "        }\n",
    "\n",
    "        fg = 500\n",
    "        spl_cdes = {}\n",
    "        for k in cmmn_label_groups:\n",
    "            cmmn_labels = cmmn_label_groups[k]\n",
    "            for l in cmmn_labels:\n",
    "                spl_cdes[l] = str(fg)\n",
    "            fg += 1\n",
    "\n",
    "        df[\"nominal__circumstance\"] = 0\n",
    "        eval_df[\"nominal__circumstance\"] = 0\n",
    "\n",
    "        def add_ctype_to_gen_f(row):\n",
    "            cols = [\"nominal__customer_type\", \"nominal__extra_travel\", \"nominal__direction\"]\n",
    "            icde = \"\"\n",
    "            for c in cols:\n",
    "                icde += str(row[c])\n",
    "            row[\"nominal__circumstance\"] = icde\n",
    "            if icde in spl_cdes:\n",
    "                row[\"nominal__circumstance\"] = spl_cdes[icde]\n",
    "            return row\n",
    "\n",
    "        df = df.apply(add_ctype_to_gen_f, axis=1)\n",
    "        eval_df = eval_df.apply(add_ctype_to_gen_f, axis=1)\n",
    "        mapped_x = list(\n",
    "            np.union1d(\n",
    "                df.nominal__circumstance.unique(), eval_df.nominal__circumstance.unique()\n",
    "            )\n",
    "        )\n",
    "        mapped_x_cde = {x: i for i, x in enumerate(mapped_x)}\n",
    "\n",
    "        def change_str_to_ord_int(row):\n",
    "            row[\"nominal__circumstance\"] = mapped_x_cde[row[\"nominal__circumstance\"]]\n",
    "            return row\n",
    "\n",
    "        df = df.apply(change_str_to_ord_int, axis=1)\n",
    "        eval_df = eval_df.apply(change_str_to_ord_int, axis=1)\n",
    "        return df, eval_df\n",
    "\n",
    "\n",
    "    data, eval_data = add_cust_type_to_trunc_features(data, eval_data)\n",
    "    return data,eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "520be7cf-20f0-4518-8c1e-e10a44bb7c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elim_redund(df,eval_df):\n",
    "    redundant = [\n",
    "        \"nominal__restaurant_type\",\n",
    "        \"binary__offer_expiration\",\n",
    "        \"binary__prefer_home_food\",\n",
    "        \"binary__is_foodie\",\n",
    "        \"binary__prefer_western_over_chinese\",\n",
    "        \"binary__cooks_regularly\",\n",
    "        \"binary__restuarant_opposite_direction_house\",\n",
    "        \"binary__restuarant_same_direction_house\",\n",
    "        \"ordinal__restaur_spend_greater_than20\",\n",
    "        \"ordinal__restaur_spend_less_than20\",\n",
    "        \"binary__travelled_more_than_15mins_for_offer\",\n",
    "        \"binary__travelled_more_than_25mins_for_offer\",\n",
    "        \"nominal__customer_type\",\n",
    "    ]\n",
    "    nr_data = df.drop(redundant, axis=1)\n",
    "    nr_eval_data = eval_df.drop(redundant, axis=1)\n",
    "    return redundant,nr_data,nr_eval_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
